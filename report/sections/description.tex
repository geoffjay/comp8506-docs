\section{Project Description}\label{sec:desc}

  \subsection{Background}\label{sec:desc-bg}

    \subsubsection{Student}\label{sec:desc-bg-student}

      Geoff Johnson is a student of the BCIT Computer Systems Bachelor's degree.
      Prior to this he obtained a diploma of Computer Systems Technology from
      Camosun college in Victoria in 2001, as well as a diploma in Robotics and
      Automation Technology from BCIT in 2006.

      For the past 10 years Geoff has worked in Burnaby for the fluid mechanics
      research and development company Coanda where he is responsible for managing
      the IT operations, as well as managing and contributing to control system
      software development. This software is used to interface with industrial
      instrumentation, perform automated feedback control of plant processes,
      and capture data to log files at upwards of 100,000 samples per second.

    \subsubsection{Company}\label{sec:desc-bg-company}

      Coanda is an engineering research and development company specializing in
      industrial fluid dynamics and related technologies. They have been in
      business for close to 20 years providing services in process flow modelling,
      design optimization, and custom instrumentation to name a few. For more
      information their website is \url{https://www.coanda.ca}.

      Much of the work performed by the company is lab based in nature, these
      projects commonly take form in a research lab where the majority
      encorporate data acquisition and control systems. These projects need to
      measure a variety of signal types (temperature, flow rate, density,
      humidity, image, and video to name a few), control industrial systems
      using different types of actuators, and reliably and accurately log all
      of the data for these process values.

      The software developed in this project performs all of these functions
      and reduces the cost to develop any individual system, in a year there
      can be as many as 50 unique control systems so the savings seen by
      reducing the time taken to develop these has a large operational, and
      financial impact.

    \subsubsection{History}\label{sec:desc-bg-project}

      The project that was proposed, and is the subject of this report, was the
      reproduction of an existing piece of software used to measure and control
      physical processes. The software that this project seeks to improve upon
      is comprised of two software projects, a library of objects that abstract
      components of a data acquisition and control system, and a GUI application
      that functions as an operator panel.

      The first is a library of objects that abstracts the data acquisition
      hardware as various types of measurement channels, it handles logging
      to both CSV files as well as databases, feedback control loop
      calculations using the PID (Proportional Integral Derivative) equation,
      and handles the creation of object hierarchy through a class builder
      using XML as the input. This library, libcld, is an open source project
      hosted in a git repository at \url{https://github.com/geoffjay/libcld}.

      The second piece of software is a user facing GUI application developed for
      the GNOME system. It interfaces with libcld, hereafter referred to as
      CLD (Control, Logging, and Data Acquisition), using a single context to
      provide the data necessary to update the interface view. This application
      has also been released as open source and is hosted in a git repository
      at \url{https://github.com/coanda/dactl}.

  \subsection{Proposed Solution}\label{sec:desc-soln}

    Performing all of the work in a single application for data acquisition,
    feedback control, and data logging is open to a variety of issues. Doing so
    limits hardware scalability, places critical operations onto systems running
    desktop software, and ties high reliability functions like plant control in
    the same process as the front end that the end user operates. These are just
    a few of the more obvious problems.

    The system being proposed in this document is one that implements all of
    the hardware access and logging functionality as micro-services that run as
    processes detached from the plant operators that interact with the GUI
    application. This server software will use an existing library to interface
    with the data acquisition hardware as well as perform the control and
    logging tasks, this aspect of the software is a reimplementation of work
    that already exists in an application that is currently in use.

    New functionality that is planned is the definition and minimal
    implementation of a REST API, a messaging system for streaming data, and a
    configuration definition to allow for the reconfiguration of the daemon. The
    REST API is meant to provide a means for clients to make simple requests of
    the system like read a single measurement value or change a property value
    of one of the internal objects. ZeroMQ is to be used to implement the
    messaging system, it has several high level features including publish and
    subscribe socket types that can be used for a streaming data system, a
    request/reply structure for a higher performance version of the REST API,
    and a structure for bridging communications to allow for the reconfiguration
    of the server as a proxy or slave node in a distributed system.

  \subsection{Alternative Solutions}\label{sec:desc-alt}

    Distributed Control Systems software is typically tied to hardware such as
    Programmable Logic Controller (PLC) and Programmable Automation Controller
    (PAC) hardware. While the cost of these types of units, typically \$500 to
    \$1000 and up, has come down in recent years it is still more than that of
    commodity hardware that the system proposed will be designed for, a goal of
    this project is to be able to deploy a system on embedded targets that are
    more prevalent now, such as a RaspberryPI or BeagleBone. The cost of these
    embedded computers ranges from \$10 to \$100, but even if a typical desktop
    computer running the Linux operating system is used the cost can be as low
    as a couple of hundred dollars.

    In the case of a PLC or a PAC programming is a non-trivial task that is
    determined by the number of I/O points and the hourly cost of a developer.
    Assuming a rate of \$125 per hour, the cost of engineering services is
    around \$440 per point\cite{Automation2014}. This cost is not insignificant
    for many users who wish to create data acquisition and control systems, and
    not everyone requires the robustness of an industrial grade PLC or PAC.
    The barrier of entry is not just the cost of the hardware or the
    development time either, often the software to program such devices is
    several thousands of dollars and in many cases requires a great deal of
    training to be able to use.

    Other open source offerings are available, Tango, FreeDCS, and Proview were
    all evaluated. Each has its own limitations that made it unappealing to the
    client. Tango has a CORBA framework that was seen as a limiting factor, but
    did offer many features built in including the ability to communicate over
    OLE for Process Control (OPC) making it possible to work with almost any
    existing DCS hardware and software. In the end the learning curve to be
    able to produce control systems using Tango made it a non-viable option. A
    goal of the development of OpenDCS is to be able to create control systems
    without writing any code, but still offer the ability to develop components
    using minimal effort and it was felt that this wasn't offered there.

    FreeDCS offers some ability to construct control systems components, such
    as a PID controller, and Human Machine Interfaces (HMI) to tie in with
    those.  The offering is limited though and cannot do many things that this
    project seeks to address, such as the ability to log data to a variety of
    storage backends. It was also unclear from the documentation how to create
    new data acquisition sources and controllers.

    Proview has several nice GUI features that may have been useful, but at the
    time of this writing the installation guide targets Fedora 11 with a kernel
    version of 2.6.26.5. It may have been possible to compile this software
    with a more modern version of the distribution, but it was also unclear how
    much flexibility the system offered regarding expanding to different data
    acquistion hardware and storage database types. It appeared to be none at
    all.

  \subsection{Innovation}\label{sec:desc-innovation}

    The proposed project is innovative in a number of ways, a couple of the
    requirements identified in an initial stakeholder review made it clear where
    significant effort would need to be placed in order to address these. One of
    these was that it needed to have an object mapping that allowed for the
    ability to easily and extensibly add and encorporate new objects into the
    system by simply adding a new object and having the configuration and data
    model recognize them. It needed to have a plugin system that exposed the
    networking subsystems in a convenient manner, and again be configurable in a
    way that was consistent with the existing system. Also, that a message queue
    be part of the system that would allow for a variety of new components that
    are commonplace in distributed systems could be added, such as message
    brokers, load balancers, and efficient data distribution.

    The configuration system was designed and developed in such a way that not
    all future extensions of it would need to be known, by having classes
    responsible for registering these new configuration managers this was
    achieved. It allows for new plugins and new systems to be added without
    modifying the primary code base, for instance plugins that are developed by
    developers can have develop their own configuration systems that allow for a
    single holistic configuration of the application that is exposed as a single
    data model without ever modifying a line of code in this project.

    Another area of innovation that is worth noting is the plugin system that is
    used by the various distributed components to do all of the heavy lifting of
    the systems. Each subsystem is only responsible for loading the
    configurations into a single data model, exposing various core components to
    the plugins, and providing a consistent API over REST and ZeroMQ. This
    design allows for developers to construct new plugins that read and write
    data from obscure hardware devices, perform new methods of logging the data
    acquired by these, and performing feedback calculations for the purpose of
    controlling hardware. This was demonstrated in the project through the
    development of a plugin that generated data for the purpose of testing the
    networking, another that consumed that data and calculated statistics on the
    data transfered over ZeroMQ, and another that consumed that same data and
    performed calculations on it to be then piped out of another publishing
    message queue.

  \subsection{Complexity}\label{sec:desc-complexity}

    Similar to the previous section on innovation the key areas of complexity
    that needed to be addressed during design and development were the
    configuration and plugin systems. The desire of the project was to not know
    everything up front that could be done with it, that is very much in the
    spirit of how the client performs their development work. In order to be
    able to develop new plugins for data acquisition, data logging, and feedback
    control it would be necessary to be able to configure these and get them
    into the system in a predictable manner. Without this there would be far too
    much difficulty to the end user when new plugins needed to be integrated
    into a system that they are already familiar with.

    As it turned out, getting a single configuration object in the services in
    order to build a single \emph{Meta} configuration built up of other smaller
    ones that described the data acquisition and logging, control system,
    networking, and plugin definitions was a complex task. Each needed to be
    loaded from a predictable and flexible location, and registered with the
    primary configuration object. The development of the plugin system presented
    it's own unique challenges as well, the different services that exposed very
    different functionality had to have an underlying structure and this ended
    up being a considerable effort without repeating code and introducing future
    complexity during time spent refactoring and maintaining it. Both of these
    elements of the project also introduced a problem that ended up being a lot
    more effort to deal with than initially understood, they required a complex
    build system that would set the project installation directories up without
    requiring any additional effort by the user at the time of deployment.
